name: Advanced Testing Framework

on:
  push:
    branches: [ main, develop, feature/*, hotfix/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run comprehensive tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Test scope to run'
        required: false
        default: 'standard'
        type: choice
        options:
          - 'smoke'        # Quick smoke tests only
          - 'standard'     # Standard test suite
          - 'comprehensive' # Full test suite including performance
          - 'regression'   # Regression tests for specific scenarios

env:
  RUBY_VERSION: '3.0'
  NODE_VERSION: '18'
  TEST_RESULTS_PATH: 'test/results'
  COVERAGE_PATH: 'test/coverage'

jobs:
  # Test planning and matrix configuration
  test-planning:
    name: Test Planning & Matrix Setup
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.matrix.outputs.matrix }}
      test-scope: ${{ steps.scope.outputs.scope }}
      should-run-performance: ${{ steps.conditions.outputs.performance }}
      should-run-security: ${{ steps.conditions.outputs.security }}
      changed-components: ${{ steps.changes.outputs.components }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect changes
        id: changes
        uses: dorny/paths-filter@v3
        with:
          filters: |
            ruby:
              - '**/*.rb'
              - '**/*.gemspec'
              - 'Gemfile*'
            theme:
              - '_layouts/**'
              - '_includes/**'
              - '_sass/**'
              - 'assets/**'
            config:
              - '_config*.yml'
              - 'frontmatter.json'
            ci:
              - '.github/workflows/**'
              - 'test/**'
              - 'scripts/**'
            docs:
              - '**/*.md'
              - 'docs/**'

      - name: Determine test scope
        id: scope
        run: |
          if [[ "${{ github.event_name }}" == "schedule" ]]; then
            echo "scope=comprehensive" >> $GITHUB_OUTPUT
          elif [[ "${{ inputs.test_scope }}" != "" ]]; then
            echo "scope=${{ inputs.test_scope }}" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "scope=standard" >> $GITHUB_OUTPUT
          else
            echo "scope=standard" >> $GITHUB_OUTPUT
          fi

      - name: Configure test matrix
        id: matrix
        run: |
          case "${{ steps.scope.outputs.scope }}" in
            smoke)
              echo 'matrix={"ruby":["3.0"],"os":["ubuntu-latest"]}' >> $GITHUB_OUTPUT
              ;;
            comprehensive)
              echo 'matrix={"ruby":["2.7","3.0","3.1","3.2"],"os":["ubuntu-latest","macos-latest"]}' >> $GITHUB_OUTPUT
              ;;
            *)
              echo 'matrix={"ruby":["2.7","3.0","3.2"],"os":["ubuntu-latest"]}' >> $GITHUB_OUTPUT
              ;;
          esac

      - name: Set test conditions
        id: conditions
        run: |
          # Performance tests
          if [[ "${{ steps.scope.outputs.scope }}" == "comprehensive" ]] || [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "performance=true" >> $GITHUB_OUTPUT
          else
            echo "performance=false" >> $GITHUB_OUTPUT
          fi
          
          # Security tests
          if [[ "${{ steps.changes.outputs.ruby }}" == "true" ]] || [[ "${{ steps.scope.outputs.scope }}" == "comprehensive" ]]; then
            echo "security=true" >> $GITHUB_OUTPUT
          else
            echo "security=false" >> $GITHUB_OUTPUT
          fi

  # Enhanced test execution with better reporting
  comprehensive-tests:
    name: Tests (Ruby ${{ matrix.ruby }} on ${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    needs: test-planning
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.test-planning.outputs.test-matrix) }}
    timeout-minutes: 20
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: ${{ matrix.ruby }}
          bundler-cache: true

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install system dependencies
        run: |
          if [[ "${{ matrix.os }}" == "ubuntu-latest" ]]; then
            sudo apt-get update
            sudo apt-get install -y jq xmlstarlet html-xml-utils
          elif [[ "${{ matrix.os }}" == "macos-latest" ]]; then
            brew install jq xmlstarlet html-xml-utils
          fi

      - name: Create test directories
        run: |
          mkdir -p ${{ env.TEST_RESULTS_PATH }}
          mkdir -p ${{ env.COVERAGE_PATH }}

      - name: Make scripts executable
        run: chmod +x scripts/*.sh test/*.sh

      - name: Run test suite with enhanced reporting
        run: |
          # Create test session metadata
          cat > ${{ env.TEST_RESULTS_PATH }}/session-metadata.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "ruby_version": "${{ matrix.ruby }}",
            "os": "${{ matrix.os }}",
            "github_ref": "${{ github.ref }}",
            "github_sha": "${{ github.sha }}",
            "test_scope": "${{ needs.test-planning.outputs.test-scope }}",
            "workflow_run_id": "${{ github.run_id }}"
          }
          EOF
          
          # Determine test suites based on scope
          case "${{ needs.test-planning.outputs.test-scope }}" in
            smoke)
              TEST_SUITES="core"
              ;;
            comprehensive)
              TEST_SUITES="all"
              ;;
            *)
              TEST_SUITES="core,deployment"
              ;;
          esac
          
          # Run tests with multiple output formats
          ./test/test_runner.sh --suites "$TEST_SUITES" --verbose --format json --environment ci --skip-remote
          
          # Copy results to expected locations for compatibility
          cp test/reports/test_report.json ${{ env.TEST_RESULTS_PATH }}/test-results.json || echo "No JSON report found"
          ./test/test_runner.sh --suites "$TEST_SUITES" --format xml --environment ci --skip-remote > ${{ env.TEST_RESULTS_PATH }}/test-results.xml || echo "XML generation failed"
          ./test/test_runner.sh --suites "$TEST_SUITES" --format html --environment ci --skip-remote > ${{ env.TEST_RESULTS_PATH }}/test-report.html || echo "HTML generation failed"

      - name: Generate test summary
        if: always()
        run: |
          # Extract key metrics from test results
          if [[ -f "${{ env.TEST_RESULTS_PATH }}/test-results.json" ]]; then
            echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
            echo "**Platform**: Ruby ${{ matrix.ruby }} on ${{ matrix.os }}" >> $GITHUB_STEP_SUMMARY
            echo "**Test Scope**: ${{ needs.test-planning.outputs.test-scope }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Parse JSON results for summary
            jq -r '
              "| Test Category | Status | Tests | Passed | Failed | Duration |",
              "|---------------|--------|--------|--------|--------|----------|",
              (.test_categories // [] | map(
                "| " + .name + " | " + 
                (if .status == "passed" then "✅" else "❌" end) + " | " +
                (.total_tests // 0 | tostring) + " | " +
                (.passed_tests // 0 | tostring) + " | " +
                (.failed_tests // 0 | tostring) + " | " +
                (.duration // "N/A") + " |"
              ) | join("\n"))
            ' ${{ env.TEST_RESULTS_PATH }}/test-results.json >> $GITHUB_STEP_SUMMARY || echo "Failed to parse test results"
          fi

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.ruby }}-${{ matrix.os }}
          path: |
            ${{ env.TEST_RESULTS_PATH }}/
            ${{ env.COVERAGE_PATH }}/
          retention-days: 30

      - name: Upload coverage to Codecov
        if: matrix.ruby == '3.0' && matrix.os == 'ubuntu-latest'
        uses: codecov/codecov-action@v3
        with:
          files: ${{ env.COVERAGE_PATH }}/coverage.xml
          flags: unittests
          name: codecov-umbrella

  # Performance benchmarking (only when needed)
  performance-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [test-planning, comprehensive-tests]
    if: needs.test-planning.outputs.should-run-performance == 'true'
    timeout-minutes: 15
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: ${{ env.RUBY_VERSION }}
          bundler-cache: true

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y time jq

      - name: Make scripts executable
        run: chmod +x test/*.sh scripts/*.sh

      - name: Run performance benchmarks
        run: |
          mkdir -p ${{ env.TEST_RESULTS_PATH }}
          ./test/test_performance.sh --format json > ${{ env.TEST_RESULTS_PATH }}/performance-results.json

      - name: Performance regression check
        run: |
          # Compare with baseline if available
          if [[ -f "baseline-performance.json" ]]; then
            echo "## Performance Comparison" >> $GITHUB_STEP_SUMMARY
            echo "Comparing current results with baseline..." >> $GITHUB_STEP_SUMMARY
            # Add performance comparison logic here
          fi

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: ${{ env.TEST_RESULTS_PATH }}/performance-results.json

  # Enhanced security scanning
  security-scan:
    name: Security Analysis
    runs-on: ubuntu-latest
    needs: test-planning
    if: needs.test-planning.outputs.should-run-security == 'true'
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: ${{ env.RUBY_VERSION }}
          bundler-cache: true

      - name: Install security tools
        run: |
          gem install bundler-audit brakeman
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Make scripts executable
        run: chmod +x test/*.sh

      - name: Run security tests
        run: |
          mkdir -p ${{ env.TEST_RESULTS_PATH }}
          ./test/test_security.sh --format json > ${{ env.TEST_RESULTS_PATH }}/security-results.json

      - name: Security summary
        if: always()
        run: |
          echo "## Security Scan Results" >> $GITHUB_STEP_SUMMARY
          if [[ -f "${{ env.TEST_RESULTS_PATH }}/security-results.json" ]]; then
            # Parse and display security results
            jq -r '.security_checks[] | "- " + .name + ": " + .status' ${{ env.TEST_RESULTS_PATH }}/security-results.json >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload security results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-results
          path: ${{ env.TEST_RESULTS_PATH }}/security-results.json

  # Test result aggregation and reporting
  test-report:
    name: Test Report & Analysis
    runs-on: ubuntu-latest
    needs: [test-planning, comprehensive-tests, performance-tests, security-scan]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/

      - name: Install analysis tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq xmlstarlet
          pip install --user json2html

      - name: Aggregate test results
        run: |
          mkdir -p consolidated-results
          
          # Combine all JSON results
          find artifacts/ -name "*.json" -type f | while read -r file; do
            echo "Processing: $file"
            cp "$file" "consolidated-results/$(basename "$(dirname "$file")")-$(basename "$file")"
          done
          
          # Create consolidated report
          echo '{"test_run_summary": {}}' > consolidated-results/master-report.json

      - name: Generate HTML dashboard
        run: |
          # Create a comprehensive HTML dashboard
          cat > consolidated-results/dashboard.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>zer0-mistakes Test Dashboard</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 20px; }
                  .header { background: #f4f4f4; padding: 20px; border-radius: 8px; }
                  .section { margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 8px; }
                  .passed { color: green; font-weight: bold; }
                  .failed { color: red; font-weight: bold; }
                  .warning { color: orange; font-weight: bold; }
                  table { width: 100%; border-collapse: collapse; }
                  th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                  th { background-color: #f2f2f2; }
              </style>
          </head>
          <body>
              <div class="header">
                  <h1>🧪 zer0-mistakes Test Dashboard</h1>
                  <p>Generated: $(date)</p>
                  <p>Commit: ${{ github.sha }}</p>
                  <p>Branch: ${{ github.ref }}</p>
              </div>
              <div class="section">
                  <h2>Test Results Overview</h2>
                  <p>Comprehensive test results will be displayed here...</p>
              </div>
          </body>
          </html>
          EOF

      - name: Upload consolidated results
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-test-results
          path: consolidated-results/
          retention-days: 90

      - name: Post results to PR (if applicable)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Generate PR comment with test results
            let comment = '## 🧪 Test Results Summary\n\n';
            comment += `**Commit**: ${context.sha.substring(0, 7)}\n`;
            comment += `**Branch**: ${context.payload.pull_request.head.ref}\n\n`;
            
            // Add test status
            comment += '### Test Status\n';
            comment += '| Component | Status |\n';
            comment += '|-----------|--------|\n';
            comment += '| Unit Tests | ${{ needs.comprehensive-tests.result == "success" && "✅" || "❌" }} |\n';
            comment += '| Security | ${{ needs.security-scan.result == "success" && "✅" || "❌" }} |\n';
            comment += '| Performance | ${{ needs.performance-tests.result == "success" && "✅" || "⚠️ Skipped" }} |\n\n';
            
            comment += '📊 [View detailed results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Final status check
  test-status:
    name: Test Status Check
    runs-on: ubuntu-latest
    needs: [comprehensive-tests, performance-tests, security-scan]
    if: always()
    steps:
      - name: Evaluate test results
        run: |
          echo "## 🧪 Overall Test Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Check critical test results
          if [[ "${{ needs.comprehensive-tests.result }}" == "success" ]]; then
            echo "✅ **Comprehensive Tests**: Passed" >> $GITHUB_STEP_SUMMARY
            tests_passed=true
          else
            echo "❌ **Comprehensive Tests**: Failed" >> $GITHUB_STEP_SUMMARY
            tests_passed=false
          fi
          
          if [[ "${{ needs.security-scan.result }}" == "success" ]]; then
            echo "✅ **Security Scan**: Passed" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.security-scan.result }}" == "skipped" ]]; then
            echo "⚠️ **Security Scan**: Skipped" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Security Scan**: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ "${{ needs.performance-tests.result }}" == "success" ]]; then
            echo "✅ **Performance Tests**: Passed" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.performance-tests.result }}" == "skipped" ]]; then
            echo "⚠️ **Performance Tests**: Skipped" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Performance Tests**: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Final determination
          if [[ "$tests_passed" == "true" ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "🎉 **Overall Status**: All critical tests passed!" >> $GITHUB_STEP_SUMMARY
            exit 0
          else
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "💥 **Overall Status**: Critical tests failed!" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi
